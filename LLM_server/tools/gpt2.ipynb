{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "filenames = ['AR22.pdf', 'AR21.pdf', 'AR20.pdf','AR19.pdf', 'AR18-19.pdf', 'AR17-18.pdf','AR16-17.pdf','AR15-16.pdf','AR14-15.pdf','AR13-14.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text_with_sentences(preprocessed_text, max_tokens=105):\n",
    "    tokens = word_tokenize(preprocessed_text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:  # If there are remaining tokens\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "def extract_and_preprocess_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        doc = fitz.open(pdf_path)\n",
    "        # Extract text from each page\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        \n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Failed to extract text from {pdf_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "def embed(text_chunks, nums, model_name='gpt2', device='cpu'):\n",
    "    # Load pre-trained GPT-2 tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2Model.from_pretrained(model_name)\n",
    "    # Move model to specified device\n",
    "    model.to(device)\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode text chunks\n",
    "    encoded_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        # Tokenize the chunk\n",
    "        tokens = tokenizer.encode(chunk)\n",
    "        nums.append(len(tokens))\n",
    "        # Truncate tokens if exceeds max length\n",
    "        if len(tokens) > tokenizer.model_max_length:\n",
    "            tokens = tokens[:tokenizer.model_max_length]\n",
    "        # Convert tokens to PyTorch tensor and add to encoded_chunks list\n",
    "        encoded_chunks.append(torch.tensor(tokens).unsqueeze(0))\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in encoded_chunks:\n",
    "            chunk = chunk.to(device)\n",
    "            outputs = model(chunk)\n",
    "            # Extract hidden states (last hidden state)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            # Take mean of hidden states across tokens to get chunk embedding\n",
    "            chunk_embedding = torch.mean(hidden_states, dim=1)\n",
    "            embeddings.append(chunk_embedding)\n",
    "\n",
    "    # Concatenate embeddings along the first dimension to get a single tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    return embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "685\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\LLM1\\\\webs\\\\data\"\n",
    "text_chunks = []\n",
    "nums = []\n",
    "allembeddings = []\n",
    "for i in filenames:\n",
    "    text = extract_and_preprocess_from_pdf(file_path+\"\\\\\"+i)\n",
    "    chunks = chunk_text_with_sentences(text)\n",
    "    text_chunks.extend(chunks)\n",
    "    break\n",
    "nums = []\n",
    "allembeddings.extend(embed(text_chunks,nums))\n",
    "print(len(text_chunks))\n",
    "print(len(allembeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n",
      "685\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(max(nums))\n",
    "print(len(allembeddings))\n",
    "print(len(allembeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage\n",
    "\n",
    "import pickle\n",
    "# import fitz\n",
    "\n",
    "# Saving allembeddings and token_chunks\n",
    "with open('allembeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(allembeddings, f)\n",
    "\n",
    "with open('text_chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(text_chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allembeddings= 685\n",
      "loaded_textchunks= 685\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('allembeddings.pkl','rb') as f:\n",
    "    loaded_allembeddings = pickle.load(f)\n",
    "with open('text_chunks.pkl', 'rb') as f:\n",
    "    loaded_textchunks = pickle.load(f)\n",
    "print(\"allembeddings=\",len(loaded_allembeddings))\n",
    "print(\"loaded_textchunks=\",len(loaded_textchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[392, 4, 119, 619, 93]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_nearest_embeddings(query_embedding, model_name='gpt2', device='cpu', top_k=5):\n",
    "    \n",
    "    with open('allembeddings.pkl','rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    # Convert lists of tensors to a single tensor\n",
    "    # query_embedding = torch.stack(query_embedding).to(device)\n",
    "    embeddings = torch.stack(embeddings).to(device)\n",
    "\n",
    "    # Move tensors to the specified device\n",
    "    query_embedding = query_embedding.to(device)\n",
    "    embeddings = embeddings.to(device)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding.cpu().numpy(), embeddings.cpu().numpy())[0]\n",
    "\n",
    "    # Find the top_k nearest embeddings\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    # top_k_similarities = similarities[top_k_indices]\n",
    "\n",
    "    return top_k_indices.tolist()\n",
    "\n",
    "query_nums=[]\n",
    "query = \"Who is the current director of ICAR-CRIDA, and who were the members of the editorial committee for the 2022 annual report?\"\n",
    "# print(embed([query],query_nums)[0])\n",
    "nearest_ids = find_nearest_embeddings(embed([query],query_nums), model_name='gpt2', device='cpu', top_k=5)\n",
    "print(nearest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /home/sharan/.local/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/sharan/.local/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sharan/.local/lib/python3.11/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/sharan/.local/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sharan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:  \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "  \n",
    "def generate_answer(query: str, context: str, model_name: str = 'gpt2', max_length: int = 1024) -> str:\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare the input text\n",
    "    input_text = f\"Context: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate the output\n",
    "    output_ids = model.generate(input_ids, \n",
    "                                max_length=max_length, \n",
    "                                num_return_sequences=1,\n",
    "                                no_repeat_ngram_size=2, \n",
    "                                temperature=0.5,\n",
    "                                top_p=0.9,\n",
    "                                do_sample=True,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the output\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-process the output to extract the answer part\n",
    "    # answer = answer.split(\"Answer:\")[1].strip()\n",
    "    answer = answer.split(\"Answer:\")[1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    # Tokenize the answer into sentences\n",
    "    sentences = sent_tokenize(answer)\n",
    "\n",
    "    # Reconstruct the answer without the last incomplete sentence\n",
    "    complete_answer = ' '.join(sentences[:-1]) if not answer.endswith('.') else answer\n",
    "    return complete_answer\n",
    "\n",
    "\n",
    "def get_context(nearest_ids):\n",
    "    with open('text_chunks.pkl', 'rb') as f:\n",
    "        loaded_chunks = pickle.load(f)\n",
    "\n",
    "    # nearest_ids.sort()\n",
    "    chunks = [loaded_chunks[i] for i in nearest_ids]\n",
    "    # for i,j in enumerate(chunks):\n",
    "    #     print(i,j)\n",
    "    context = \" \".join([i for i in chunks])\n",
    "    # print(\"context: \", context)\n",
    "    return context\n",
    "\n",
    "# print(\"Query: \",query)\n",
    "# context = get_context(nearest_ids)\n",
    "answer = generate_answer(\"who are you\", \"context\")\n",
    "print(\"answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
